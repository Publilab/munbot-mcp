# An√°lisis Completo de Estrategia Conversacional MCP

## üìã Resumen Ejecutivo

El documento presenta una estrategia integral para transformar un chatbot actual basado en MCP y l√≥gica if-else en un sistema conversacional avanzado que aprovecha **mistral-7b-instruct** como motor LLM principal, manteniendo la confiabilidad del sistema estructurado actual.

### Objetivo Principal
Implementar un **orquestador h√≠brido inteligente** que combine:
- La precisi√≥n del sistema basado en reglas para tareas cr√≠ticas
- La fluidez conversacional del LLM para interacciones naturales
- Un sistema de fallback multicapa robusto

---

## üèóÔ∏è Arquitectura Propuesta

### Componentes Principales del Sistema H√≠brido

#### 1. **MCP Hybrid Orchestrator** (`orchestrator.py`)
- **Funci√≥n**: Punto de entrada central para todas las consultas
- **Responsabilidad**: Orquestar el flujo conversacional completo
- **Tecnolog√≠a**: FastAPI con integraci√≥n Redis y PostgreSQL

#### 2. **Decision Layer** (Impulsado por `mistral-7b-instruct`)
Analiza cada entrada del usuario para extraer:
- **Intenci√≥n**: Objetivo del usuario
- **Entidades**: Datos clave (n√∫meros de pedido, fechas, etc.)
- **Confianza**: Nivel de certeza del an√°lisis (0-1)
- **Sentimiento**: Estado emocional del usuario

#### 3. **Rule-Based System** (Servidores MCP)
- **complaints-mcp**: Gesti√≥n de reclamos y denuncias
- **scheduler-mcp**: Sistema de agendamiento de citas
- **llm_docs-mcp**: Consulta de documentaci√≥n municipal

#### 4. **LLM Generation Engine** (`mistral-7b-instruct`)
- Genera respuestas conversacionales fluidas
- Maneja preguntas abiertas y charlas informales
- Act√∫a como fallback cuando el sistema de reglas no tiene respuesta

#### 5. **Response Engine**
- Unifica respuestas de reglas y LLM
- Entrega respuesta final al usuario
- Actualiza el contexto conversacional

---

## üîÑ Flujos de Datos Conversacionales

### Flujo General
1. **Recepci√≥n**: Usuario env√≠a mensaje ‚Üí `orchestrator.py`
2. **An√°lisis**: Decision Layer (mistral-7b-instruct) analiza intenci√≥n
3. **Evaluaci√≥n**: Orquestador eval√∫a confianza y enruta:
   - **Alta confianza + intenci√≥n conocida** ‚Üí Servidor MCP espec√≠fico
   - **Confianza media + intenci√≥n conversacional** ‚Üí LLM Generation Engine
   - **Baja confianza** ‚Üí Sistema de fallback
4. **Procesamiento**: Motor seleccionado procesa la solicitud
5. **Respuesta**: Response Engine env√≠a respuesta final
6. **Actualizaci√≥n**: Contexto conversacional actualizado

### Flujos Espec√≠ficos por Microservicio

#### **complaints-mcp** (Gesti√≥n de Reclamos)
```
Usuario: "Necesito poner un reclamo"
‚îú‚îÄ‚îÄ An√°lisis de intenci√≥n ‚Üí complaint-registrar_reclamo
‚îú‚îÄ‚îÄ Recolecci√≥n de datos personales (con validaci√≥n)
‚îú‚îÄ‚îÄ Captura del contenido del reclamo
‚îú‚îÄ‚îÄ Registro en base de datos
‚îú‚îÄ‚îÄ Confirmaci√≥n visual al usuario
‚îú‚îÄ‚îÄ Env√≠o por email
‚îî‚îÄ‚îÄ Despedida
```

#### **scheduler-mcp** (Agendamiento)
```
Usuario: "Necesito pedir una hora de atenci√≥n"
‚îú‚îÄ‚îÄ An√°lisis de intenci√≥n ‚Üí scheduler-appointment_create
‚îú‚îÄ‚îÄ Consulta de horarios disponibles
‚îú‚îÄ‚îÄ Presentaci√≥n de opciones al usuario
‚îú‚îÄ‚îÄ Recolecci√≥n de datos personales
‚îú‚îÄ‚îÄ Reserva y registro
‚îú‚îÄ‚îÄ Confirmaci√≥n visual
‚îú‚îÄ‚îÄ Env√≠o por email
‚îî‚îÄ‚îÄ Despedida
```

#### **llm_docs-mcp** (Documentaci√≥n)
Dos funciones principales:
1. **B√∫squeda en documentos**: Consulta espec√≠fica en documentos municipales
2. **Generaci√≥n conversacional**: Evita fallback para consultas dentro del contexto

---

## üíª Requisitos T√©cnicos por Microservicio

### **mcp-core** (Orquestador Principal)

#### Dependencias T√©cnicas
```python
- FastAPI (API framework)
- psycopg2 (PostgreSQL)
- redis (gesti√≥n de sesiones)
- requests (comunicaci√≥n entre servicios)
- pydantic (validaci√≥n de datos)
```

#### Configuraci√≥n de Servicios
```python
MICROSERVICES = {
    "complaints-mcp": "http://complaints-mcp:7000/tools/call",
    "llm_docs-mcp": "http://llm-docs-mcp:8000/tools/call", 
    "scheduler-mcp": "http://scheduler-mcp:6001/tools/call"
}
```

#### Campos Requeridos por Tool
```python
REQUIRED_FIELDS = {
    "complaint-registrar_reclamo": ["nombre", "mail", "mensaje", "categoria", "departamento"],
    "scheduler-appointment_create": ["usu_name", "usu_mail", "usu_whatsapp", "fecha", "hora", "motiv"]
}
```

### **Integraci√≥n con Mistral-7B-Instruct**

#### Configuraci√≥n de API
```python
api_url = "https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.3"
headers = {"Authorization": f"Bearer {HF_API_TOKEN}"}
parameters = {
    "max_new_tokens": 256,
    "temperature": 0.7,
    "return_full_text": False
}
```

#### An√°lisis de Intenci√≥n
El LLM analiza cada consulta para identificar:
- `complaint-registrar_reclamo`
- `doc-buscar_fragmento_documento`
- `doc-generar_respuesta_llm`
- `scheduler-reservar_hora`
- `scheduler-appointment_create`
- `scheduler-listar_horas_disponibles`
- `scheduler-cancelar_hora`
- `scheduler-confirmar_hora`

### **Gesti√≥n de Datos**

#### PostgreSQL (Persistencia)
- **Base de datos**: `munbot`
- **Tabla principal**: `conversaciones_historial`
- **Esquemas**: `appointments.sql`, `reclamos.sql`

#### Redis (Sesiones)
- **Host**: `redis:6379`
- **Funci√≥n**: Memoria conversacional a corto plazo
- **Expiraci√≥n**: 3600 segundos (1 hora)

---

## üí¨ Gesti√≥n de Contexto y Mejores Pr√°cticas Conversacionales

### **ConversationalContextManager**

#### Caracter√≠sticas Principales
```python
class ConversationalContextManager:
    - Memoria a corto plazo: Redis (sesiones activas)
    - Memoria a largo plazo: Base de datos vectorial
    - Expiraci√≥n de sesi√≥n: 1 hora
    - L√≠mite de historial: 10 intercambios por sesi√≥n
```

#### Funciones Clave
- `get_context(session_id)`: Recupera historial conversacional
- `update_context(session_id, user_query, bot_response)`: Actualiza contexto
- `get_history_as_string(history)`: Formatea para prompts LLM

### **Personalizaci√≥n de Respuestas**
```python
async def generate_personalized_response(user_query: str, context: dict):
    user_name = context.get("user_profile", {}).get("name", "cliente")
    personalized_prompt = f"Dir√≠gete al usuario como {user_name}. {base_prompt}"
```

### **Gesti√≥n de Prompts**
- **Ubicaci√≥n**: `/prompts/` directory
- **Esquemas de herramientas**: `/tool_schemas/`
- **Plantillas contextuales**: Reemplazo de variables `{{variable}}`

---

## üõ°Ô∏è Sistema de Fallback Multicapa

### **Nivel 1: Detecci√≥n de Confianza**
```python
if confidence < HYBRID_CONFIDENCE_THRESHOLD:
    # An√°lisis de sentimiento para detectar frustraci√≥n
    sentiment = intent_analysis.get("sentiment", "neutral")
    
    if sentiment == "very_negative":
        # Escalaci√≥n inmediata a humano
        escalation_client.handle({"reason": "User frustration detected"})
```

### **Nivel 2: Respuesta Generativa Segura**
```python
# El LLM explica que no entendi√≥ pero intenta ayudar
response = llm_client.generate_safe_fallback_response(user_query, context)
```

### **Nivel 3: Escalaci√≥n Progresiva**
```python
fallback_count = context_manager.get_fallback_count(session_id)
if fallback_count >= 3:
    # Escalaci√≥n a humano tras m√∫ltiples fallos
    escalation_client.handle({"reason": "Repeated understanding failures"})
```

### **Estrategias de Recuperaci√≥n**
- **An√°lisis de sentimiento**: Detecci√≥n de frustraci√≥n del usuario
- **Conteo de fallbacks**: Escalaci√≥n autom√°tica tras 3 intentos
- **Logging detallado**: Registro de eventos para an√°lisis posterior

---

## üìä Monitoreo y KPIs

### **M√©tricas Clave de Rendimiento**

#### 1. **Tasa de Resoluci√≥n en Primer Contacto (FCR)**
- **C√°lculo**: `% consultas resueltas sin fallback / Total consultas`
- **Meta**: >85%

#### 2. **Tasa de Comprensi√≥n de Intenci√≥n**
- **C√°lculo**: `(Llamadas MCP + LLM alta confianza) / Total consultas`
- **Meta**: >90%

#### 3. **Tasa de Fallback**
- **C√°lculo**: `Activaciones de fallback / Total consultas`
- **Meta**: <10%

#### 4. **Puntuaci√≥n de Satisfacci√≥n (CSAT)**
- **Escala**: 1-5
- **Meta**: >4.0

#### 5. **Duraci√≥n Promedio de Conversaci√≥n**
- **Medici√≥n**: Turnos conversacionales o tiempo total
- **Objetivo**: Reducci√≥n progresiva manteniendo calidad

### **Herramientas de An√°lisis**
- **Dashboard**: Grafana/Kibana/Looker para visualizaci√≥n en tiempo real
- **Logging detallado**: Registro de decisiones del orquestador
- **An√°lisis de conversaciones fallidas**: Sistema de marcado y revisi√≥n manual

### **Proceso de Mejora Continua**
1. **Recopilaci√≥n**: Logs y m√©tricas de rendimiento
2. **An√°lisis semanal**: Revisi√≥n de conversaciones fallidas y KPIs
3. **Refinamiento de prompts**: Ingenier√≠a iterativa de prompts
4. **Nuevas reglas**: Creaci√≥n de servidores MCP para patrones identificados

---

## üîß Clases y Componentes T√©cnicos Principales

### **Modelos de Datos (Pydantic)**
```python
class OrchestratorInput(BaseModel)           # Entrada del orquestador
class ComplaintModel(BaseModel)              # Modelo de reclamos
class ComplaintOut(BaseModel)                # Salida de reclamos  
class AppointmentCreate(BaseModel)           # Creaci√≥n de citas
class AppointmentOut(AppointmentCreate)      # Salida de citas
class AppointmentConfirm(BaseModel)          # Confirmaci√≥n de citas
class AppointmentCancel(BaseModel)           # Cancelaci√≥n de citas
```

### **Componentes de Procesamiento**
```python
class HybridOrchestrator                     # Orquestador principal h√≠brido
class MistralClient                          # Cliente para mistral-7b-instruct
class ConversationalContextManager          # Gestor de contexto conversacional
class ComplaintRepository                    # Repositorio de reclamos
class IPWhitelistMiddleware                  # Middleware de seguridad
```

### **Funciones Utilitarias Clave**
```python
def detect_intent_llm(user_input: str)      # Detecci√≥n de intenci√≥n con LLM
def detect_intent_keywords(user_input: str) # Fallback basado en palabras clave
def route_to_service(tool: str)             # Enrutamiento a microservicios
def validate_against_schema(data, schema)   # Validaci√≥n de esquemas
def call_tool_microservice(tool, params)    # Llamadas a microservicios
def lookup_faq_respuesta(pregunta: str)     # B√∫squeda en FAQ
```

---

## ‚úÖ Cumplimiento de Criterios de √âxito

### ‚úÖ **Extracci√≥n completa de la arquitectura propuesta**
- Arquitectura h√≠brida con orquestador inteligente claramente definida
- 5 componentes principales identificados y documentados
- Flujo de datos completo mapeado

### ‚úÖ **Identificaci√≥n de todos los componentes del sistema h√≠brido**
- **mcp-core**: Orquestador principal con FastAPI
- **complaints-mcp**: Gesti√≥n de reclamos (puerto 7000)
- **scheduler-mcp**: Sistema de agendamiento (puerto 6001)  
- **llm_docs-mcp**: Consulta de documentaci√≥n (puerto 8000)
- **mistral-7b-instruct**: Motor LLM v√≠a HuggingFace API

### ‚úÖ **An√°lisis detallado de los flujos conversacionales espec√≠ficos**
- Flujo general de 6 pasos documentado
- Flujos espec√≠ficos por microservicio detallados
- L√≥gica de enrutamiento basada en confianza e intenci√≥n

### ‚úÖ **Documentaci√≥n de los requisitos t√©cnicos por microservicio**
- Dependencias Python identificadas (FastAPI, psycopg2, redis, etc.)
- Configuraci√≥n de servicios y puertos
- Campos requeridos por herramienta especificados
- Estructura de base de datos documentada

### ‚úÖ **Identificaci√≥n de las mejores pr√°cticas conversacionales propuestas**
- Gesti√≥n de contexto con memoria a corto y largo plazo
- Personalizaci√≥n de respuestas con informaci√≥n del usuario
- Sistema de prompts templatable con variables
- Limitaci√≥n de historial conversacional (10 intercambios)

### ‚úÖ **An√°lisis de la gesti√≥n de contexto y fallbacks**
- **Contexto**: Redis para sesiones activas, BD vectorial para persistencia
- **Fallbacks multicapa**: 3 niveles de escalaci√≥n progresiva
- **Detecci√≥n de frustraci√≥n**: An√°lisis de sentimiento integrado
- **Recuperaci√≥n autom√°tica**: Escalaci√≥n tras 3 intentos fallidos

---

## üéØ Conclusiones y Recomendaciones

### **Fortalezas de la Estrategia**
1. **Arquitectura bien estructurada** con separaci√≥n clara de responsabilidades
2. **Enfoque h√≠brido equilibrado** entre confiabilidad y conversacionalidad
3. **Sistema de fallback robusto** con m√∫ltiples capas de recuperaci√≥n
4. **Monitoreo integral** con KPIs espec√≠ficos y medibles
5. **Escalabilidad modular** que permite agregar nuevos microservicios

### **Implementaci√≥n Progresiva Recomendada**
1. **Fase 1**: Implementar orquestador h√≠brido b√°sico
2. **Fase 2**: Integrar gesti√≥n de contexto conversacional
3. **Fase 3**: Activar sistema de fallback multicapa  
4. **Fase 4**: Implementar monitoreo y an√°lisis de KPIs
5. **Fase 5**: Optimizaci√≥n continua basada en datos

### **Consideraciones T√©cnicas Cr√≠ticas**
- **Latencia**: Optimizar llamadas a API de Mistral para respuesta r√°pida
- **Escalabilidad**: Configurar Redis cluster para alta disponibilidad
- **Seguridad**: Implementar autenticaci√≥n robusta para APIs externas
- **Costos**: Monitorear uso de tokens de Mistral para control de gastos

La estrategia propuesta representa una evoluci√≥n natural y bien planificada del sistema actual hacia una experiencia conversacional m√°s rica y humana, manteniendo la confiabilidad operacional existente.
