---
sticker: lucide//chevron-down-square
---
# Generación Augmentada con Recuperación (RAG)

**Descripción:** La técnica de _Retrieval-Augmented Generation_ combina lo mejor de dos mundos: búsqueda de información en fuentes autorizadas + generación de lenguaje natural. En este enfoque, ante una pregunta del usuario, el sistema primero recupera documentos, fragmentos o datos relevantes de un **repositorio de conocimiento** (por ejemplo, textos de normativas, documentos PDF, bases de datos municipales) y luego alimenta esos fragmentos al LLM para que genere una respuesta fundamentada en esa información [aws.amazon.com](https://aws.amazon.com/es/what-is/retrieval-augmented-generation/#:~:text=La%20generaci%C3%B3n%20mejorada%20por%20recuperaci%C3%B3n,de%20volver%20a%20entrenar%20el). En esencia, el LLM ya no responde solo con “lo que sabe” de su entrenamiento, sino que **consulta una base de conocimiento externa y actualizada** y elabora la respuesta apoyándose en ella [aws.amazon.com](https://aws.amazon.com/es/what-is/retrieval-augmented-generation/#:~:text=La%20generaci%C3%B3n%20mejorada%20por%20recuperaci%C3%B3n,de%20volver%20a%20entrenar%20el) [aws.amazon.com](https://aws.amazon.com/es/what-is/retrieval-augmented-generation/#:~:text=Informaci%C3%B3n%20actual).

### **Ventajas:**

- **Mayor Precisión y Actualización:** Al extraer información de fuentes oficiales actualizadas, las respuestas del chatbot serán más precisas y alineadas con la realidad vigente. Se reduce el riesgo de datos obsoletos: por ejemplo, si cambia un requisito de un trámite, el bot lo reflejaría al consultar la documentación más reciente en lugar de basarse en conocimiento entrenado antiguo [aws.amazon.com](https://aws.amazon.com/es/what-is/retrieval-augmented-generation/#:~:text=Informaci%C3%B3n%20actual).    
- **Reducción de Alucinaciones:** RAG mitiga las invenciones del LLM, ya que la generación está “anclada” a contenidos concretos. El modelo tiende a usar los fragmentos recuperados para formular la respuesta, citando o parafraseando información verdadera en vez de fabricarla [aws.amazon.com](https://aws.amazon.com/es/what-is/retrieval-augmented-generation/#:~:text=El%20RAG%20es%20un%20enfoque,el%20LLM%20genera%20la%20respuesta). Esto aumenta la confianza en las respuestas entregadas.    
- **Ampliación del Alcance de Conocimiento:** Se puede cargar en la base de conocimiento una gran variedad de documentos municipales: reglamentos, preguntas frecuentes detalladas, actas, datos históricos, etc. El chatbot podría entonces contestar preguntas muy específicas (“¿Qué dice la ordenanza X sobre ruidos molestos?”) buscando exactamente ese párrafo en el texto legal y explicándolo. Incluso consultas complejas pueden ser respondidas combinando múltiples fuentes.    
- **Mantenimiento más Sencillo que FAQ Estática:** Aunque requiere mantener la base documental, actualizar un documento o agregar uno nuevo puede ser más sencillo que anticipar cada posible pregunta. Además, **no** se necesita re-entrenar el LLM para incorporar nuevos datos; simplemente se agregan a la base de conocimiento y estarán disponibles para futuras consultas[aws.amazon.com](https://aws.amazon.com/es/what-is/retrieval-augmented-generation/#:~:text=entrenamiento%20antes%20de%20generar%20una,y%20%C3%BAtiles%20en%20diversos%20contextos)[aws.amazon.com](https://aws.amazon.com/es/what-is/retrieval-augmented-generation/#:~:text=preguntas%2C%20traducir%20idiomas%20y%20completar,de%20volver%20a%20entrenar%20el). Esto resulta coste-efectivo para introducir datos específicos de la organización sin ajustar el modelo base.    
- **Transparencia y Fuente de Verdad:** Un sistema RAG bien diseñado puede incluso proveer las fuentes (ej. mostrar extractos o referencias de dónde obtuvo la información). En un contexto municipal, esto es valioso para dar respaldo a la respuesta – por ejemplo, citando la ordenanza o el comunicado oficial pertinente, lo que mejora la confianza del ciudadano en la respuesta.
    
### **Desventajas:**

- **Mayor Complejidad de Arquitectura:** Implementar RAG añade componentes adicionales. Se necesita un **motor de búsqueda** o **base de datos vectorial** para indexar y consultar los documentos relevantes [aws.amazon.com](https://aws.amazon.com/es/what-is/retrieval-augmented-generation/#:~:text=Los%20nuevos%20datos%20fuera%20del,de%20IA%20generativa%20pueden%20entender). Esto implica procesos de pre-procesamiento (parseo de documentos, generación de embeddings vectoriales, construcción de índices) y orquestación para combinar la etapa de búsqueda con la de generación.    
- **Costos Computacionales y de Almacenamiento:** Mantener un índice de vectores (por ejemplo, usando Qdrant, Weaviate, ElasticSearch, etc.) tiene requerimientos de almacenamiento para los embeddings, y cada consulta implica cálculos de similitud. Si los documentos son muy numerosos o extensos, puede requerir recursos significativos (aunque sigue siendo más barato que entrenar un modelo con todos esos datos).   
- **Latencia Adicional:** A diferencia de responder directamente de un JSON, aquí el flujo es: consulta vectorial/keyword → recuperar textos → invocar LLM con contexto. Cada consulta del usuario dispara varias operaciones, lo que puede aumentar ligeramente el tiempo de respuesta. Optimizar para que siga en rangos aceptables (p. ej. usando caché para respuestas frecuentes) será necesario.    
- **Necesidad de Actualizar la Base de Conocimiento:** Aunque no se re-entrene el modelo, sí hay que asegurar que los documentos base se mantienen al día. Si, por ejemplo, una ordenanza es derogada y se aprueba una nueva, se debe cargar el cambio en el repositorio documental. Esto requiere un flujo de mantenimiento (manual o automatizado). Afortunadamente, se pueden establecer procesos automáticos para reindexar documentos modificados periódicamente [aws.amazon.com](https://aws.amazon.com/es/what-is/retrieval-augmented-generation/#:~:text=Actualizar%20datos%20externos).
    

### **Requerimientos Técnicos:**

- **Motor de Búsqueda o Base Vectorial:** Para implementar RAG se podría integrar un servicio como **Elasticsearch** (búsqueda de texto tradicional y/o semántica) o una **base de datos de vectores** como **Qdrant** o **Pinecone** para búsqueda por similitud semántica. Por ejemplo, Qdrant permitiría almacenar embeddings de textos municipales y recuperar rápidamente los más similares a la pregunta del usuario. Los datos a indexar incluirían la FAQ existente, documentos oficiales (PDFs, DOCs convertidos a texto) y posiblemente contenido de la web municipal.
    
- **Generación de Embeddings:** Se necesita un modelo de _embeddings_ para representar preguntas y documentos en vectores numéricos comparables. OpenAI ofrece modelos como `text-embedding-ada-002` o se pueden usar alternativos open-source (Sentence-BERT, etc.). Ya en implementaciones previas se usaba Ada-002 para embeddings de FAQ [GitHub](https://github.com/danielob19/asistente-lic-bustamante/blob/3bc1c79fbb06167c6f858dfbef12263036781657/core/faq_semantica.py#L39-L47). Habría que incorporar un proceso que tome cada documento municipal y calcule sus embeddings, almacenándolos en la base vectorial.
    
- **Pipeline de Preprocesamiento:** Posiblemente un contenedor o script dedicado (por ejemplo, un flujo en `llm_docs-mcp` o un **ETL** externo) que extraiga el texto de fuentes diversas (bases de datos municipales, PDFs escaneados, etc.), limpie el texto y lo ingrese al índice. El archivo `tools/refresh_llm_docs.sh` sugiere la existencia de un mecanismo para refrescar la base de documentación. Esto puede programarse para correr periódicamente o bajo demanda cuando se carguen nuevos documentos (p. ej., vía los endpoints de administración de documentos ya disponibles[GitHub](https://github.com/Publilab/munbot-mcp/blob/dde8c1b06f1e6ec072cd2fb253579f9afd502e6f/mcp-core/orchestrator.py#L855-L864)[GitHub](https://github.com/Publilab/munbot-mcp/blob/dde8c1b06f1e6ec072cd2fb253579f9afd502e6f/mcp-core/orchestrator.py#L923-L931)).
    
- **Cambios en `llm_docs-mcp`:** Este microservicio debería encargarse de las operaciones de búsqueda. Podría exponerse un endpoint (o usar el existente `/tools/call`) para consultas tipo `doc-buscar_fragmento_documento`. Internamente, en vez de una simple búsqueda SQL como ahora, realizaría la búsqueda en la base vectorial o Elastic y devolvería los fragmentos más relevantes. Actualmente, el orquestador ya define la intención `doc-buscar_fragmento_documento` y le asigna el microservicio `llm_docs-mcp` [GitHub](https://github.com/Publilab/munbot-mcp/blob/dde8c1b06f1e6ec072cd2fb253579f9afd502e6f/mcp-core/orchestrator.py#L248-L255) [GitHub](https://github.com/Publilab/munbot-mcp/blob/dde8c1b06f1e6ec072cd2fb253579f9afd502e6f/mcp-core/orchestrator.py#L240-L248), por lo que habría que implementar la lógica correspondiente en `llm_docs-mcp` (posiblemente en su `gateway.py`). El objetivo es que cuando reciba la consulta, extraiga digamos los 3 pasajes más relevantes de la documentación [GitHub](https://github.com/Publilab/munbot-mcp/blob/dde8c1b06f1e6ec072cd2fb253579f9afd502e6f/mcp-core/orchestrator.py#L717-L725).
    
- **Cambios en `mcp-core`:** El orquestador deberá integrar esta funcionalidad de forma transparente. Dado que ya existe una función para obtener “snippets” de contexto tanto de FAQ como de documentos[GitHub](https://github.com/Publilab/munbot-mcp/blob/dde8c1b06f1e6ec072cd2fb253579f9afd502e6f/mcp-core/orchestrator.py#L322-L331) [GitHub](https://github.com/Publilab/munbot-mcp/blob/dde8c1b06f1e6ec072cd2fb253579f9afd502e6f/mcp-core/orchestrator.py#L332-L340), esa podría modificarse para usar el nuevo motor de búsqueda en lugar de (o además de) la consulta SQL sencilla. Alternativamente, el orquestador puede delegar totalmente la búsqueda al microservicio: es decir, cuando la intención sea `doc-buscar_fragmento_documento`, simplemente envía la pregunta a `llm_docs-mcp` y éste retorna los textos encontrados. Luego el orquestador podría decidir si devolver esos fragmentos tal cual o, más potente, invocar al LLM para que genere una respuesta final usando dichos fragmentos (lo que sería la intención `doc-generar_respuesta_llm`). Actualmente, el código ya contempla un flujo donde compone un prompt con los snippets de FAQ/documentos y obtiene una respuesta del LLM [GitHub](https://github.com/Publilab/munbot-mcp/blob/dde8c1b06f1e6ec072cd2fb253579f9afd502e6f/mcp-core/orchestrator.py#L717-L725) [GitHub](https://github.com/Publilab/munbot-mcp/blob/dde8c1b06f1e6ec072cd2fb253579f9afd502e6f/mcp-core/orchestrator.py#L729-L732); esto es básicamente RAG: recuperación de trozos y generación de la respuesta. Con una base documental más robusta, se continuaría ese enfoque, solo que mejorando cómo se obtienen los trozos relevantes.
    
- **Gestión de Contexto e Historial:** Es importante también ajustar el manejo de contexto conversacional. En RAG, a veces la pregunta del usuario puede ser ambigua hasta que se ve en contexto de la conversación. El `ConversationalContextManager` ya almacena el historial reciente en Redis[GitHub](https://github.com/Publilab/munbot-mcp/blob/dde8c1b06f1e6ec072cd2fb253579f9afd502e6f/docs/analisis_estrategia_conversacional_mcp.md#L2-L5), y ese historial se incluye en el prompt[GitHub](https://github.com/Publilab/munbot-mcp/blob/dde8c1b06f1e6ec072cd2fb253579f9afd502e6f/mcp-core/orchestrator.py#L717-L725). Se podría considerar también utilizar un **almacenamiento vectorial para memoria a largo plazo** (mencionado en la arquitectura propuesta[GitHub](https://github.com/Publilab/munbot-mcp/blob/dde8c1b06f1e6ec072cd2fb253579f9afd502e6f/docs/analisis_estrategia_conversacional_mcp.md#L8-L11)), de modo que incluso si el usuario retoma un tema anterior, el sistema pueda recordar qué documentos o info se usó antes. Esto requiere almacenar representaciones de la conversación en la misma base vectorial o en otra, pero es un plus avanzado.
    

**Aplicabilidad en Entorno Municipal:** La generación aumentada con recuperación es **sumamente prometedora** para un chatbot municipal. Las municipalidades manejan gran cantidad de información reglamentaria y de servicios que suele estar en documentos formales o bases de datos. RAG permitiría que el bot brinde respuestas **precisas y fundamentadas**, citando por ejemplo el artículo de una ordenanza o los pasos listados en un trámite oficial. Esto eleva la calidad del servicio, pues el ciudadano recibe información confiable y detallada, en vez de una respuesta general. Además, es escalable: si mañana se agregan nuevos servicios o normativas, se incorporan los documentos y el bot podría atender preguntas sobre ellos sin necesidad de reprogramación. En términos de viabilidad, muchas ciudades ya disponen de sitios web con sus normativas y guías; esas mismas fuentes pueden nutrir el índice de RAG. Como desafío, está la necesidad de mantener actualizado el repositorio (lo cual puede coordinarse con la unidad de comunicaciones o secretaría municipal que emita nuevos contenidos). En resumen, el enfoque RAG equilibra la **naturalidad** de las respuestas generadas con la **exactitud** de las fuentes municipales, lo que es ideal para un asistente virtual público.

_Esquema conceptual de un flujo RAG:_ _El sistema recupera información relevante de la base de conocimientos municipal (documentos, FAQs, datos) y la incorpora al prompt del LLM. El modelo genera una respuesta apoyada en esa información, mejorando precisión y actualidad de la respuesta._