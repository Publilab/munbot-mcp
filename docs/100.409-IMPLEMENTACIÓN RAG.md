---
sticker: lucide//chevron-down-square
---
# Generación Augmentada con Recuperación (RAG)

**Descripción:** La técnica de _Retrieval-Augmented Generation_ combina lo mejor de dos mundos: búsqueda de información en fuentes autorizadas + generación de lenguaje natural. En este enfoque, ante una pregunta del usuario, el sistema primero recupera documentos, fragmentos o datos relevantes de un **repositorio de conocimiento** (por ejemplo, textos de normativas, documentos PDF, bases de datos municipales) y luego alimenta esos fragmentos al LLM para que genere una respuesta fundamentada en esa información [aws.amazon.com](https://aws.amazon.com/es/what-is/retrieval-augmented-generation/#:~:text=La%20generaci%C3%B3n%20mejorada%20por%20recuperaci%C3%B3n,de%20volver%20a%20entrenar%20el). En esencia, el LLM ya no responde solo con “lo que sabe” de su entrenamiento, sino que **consulta una base de conocimiento externa y actualizada** y elabora la respuesta apoyándose en ella [aws.amazon.com](https://aws.amazon.com/es/what-is/retrieval-augmented-generation/#:~:text=La%20generaci%C3%B3n%20mejorada%20por%20recuperaci%C3%B3n,de%20volver%20a%20entrenar%20el) [aws.amazon.com](https://aws.amazon.com/es/what-is/retrieval-augmented-generation/#:~:text=Informaci%C3%B3n%20actual).

### **Ventajas:**

- **Mayor Precisión y Actualización:** Al extraer información de fuentes oficiales actualizadas, las respuestas del chatbot serán más precisas y alineadas con la realidad vigente. Se reduce el riesgo de datos obsoletos: por ejemplo, si cambia un requisito de un trámite, el bot lo reflejaría al consultar la documentación más reciente en lugar de basarse en conocimiento entrenado antiguo [aws.amazon.com](https://aws.amazon.com/es/what-is/retrieval-augmented-generation/#:~:text=Informaci%C3%B3n%20actual).    
- **Reducción de Alucinaciones:** RAG mitiga las invenciones del LLM, ya que la generación está “anclada” a contenidos concretos. El modelo tiende a usar los fragmentos recuperados para formular la respuesta, citando o parafraseando información verdadera en vez de fabricarla [aws.amazon.com](https://aws.amazon.com/es/what-is/retrieval-augmented-generation/#:~:text=El%20RAG%20es%20un%20enfoque,el%20LLM%20genera%20la%20respuesta). Esto aumenta la confianza en las respuestas entregadas.    
- **Ampliación del Alcance de Conocimiento:** Se puede cargar en la base de conocimiento una gran variedad de documentos municipales: reglamentos, preguntas frecuentes detalladas, actas, datos históricos, etc. El chatbot podría entonces contestar preguntas muy específicas (“¿Qué dice la ordenanza X sobre ruidos molestos?”) buscando exactamente ese párrafo en el texto legal y explicándolo. Incluso consultas complejas pueden ser respondidas combinando múltiples fuentes.    
- **Mantenimiento más Sencillo que FAQ Estática:** Aunque requiere mantener la base documental, actualizar un documento o agregar uno nuevo puede ser más sencillo que anticipar cada posible pregunta. Además, **no** se necesita re-entrenar el LLM para incorporar nuevos datos; simplemente se agregan a la base de conocimiento y estarán disponibles para futuras consultas[aws.amazon.com](https://aws.amazon.com/es/what-is/retrieval-augmented-generation/#:~:text=entrenamiento%20antes%20de%20generar%20una,y%20%C3%BAtiles%20en%20diversos%20contextos)[aws.amazon.com](https://aws.amazon.com/es/what-is/retrieval-augmented-generation/#:~:text=preguntas%2C%20traducir%20idiomas%20y%20completar,de%20volver%20a%20entrenar%20el). Esto resulta coste-efectivo para introducir datos específicos de la organización sin ajustar el modelo base.    
- **Transparencia y Fuente de Verdad:** Un sistema RAG bien diseñado puede incluso proveer las fuentes (ej. mostrar extractos o referencias de dónde obtuvo la información). En un contexto municipal, esto es valioso para dar respaldo a la respuesta – por ejemplo, citando la ordenanza o el comunicado oficial pertinente, lo que mejora la confianza del ciudadano en la respuesta.
    
### **Desventajas:**

- **Mayor Complejidad de Arquitectura:** Implementar RAG añade componentes adicionales. Se necesita un **motor de búsqueda** o **base de datos vectorial** para indexar y consultar los documentos relevantes [aws.amazon.com](https://aws.amazon.com/es/what-is/retrieval-augmented-generation/#:~:text=Los%20nuevos%20datos%20fuera%20del,de%20IA%20generativa%20pueden%20entender). Esto implica procesos de pre-procesamiento (parseo de documentos, generación de embeddings vectoriales, construcción de índices) y orquestación para combinar la etapa de búsqueda con la de generación.    
- **Costos Computacionales y de Almacenamiento:** Mantener un índice de vectores (por ejemplo, usando Qdrant, Weaviate, ElasticSearch, etc.) tiene requerimientos de almacenamiento para los embeddings, y cada consulta implica cálculos de similitud. Si los documentos son muy numerosos o extensos, puede requerir recursos significativos (aunque sigue siendo más barato que entrenar un modelo con todos esos datos).   
- **Latencia Adicional:** A diferencia de responder directamente de un JSON, aquí el flujo es: consulta vectorial/keyword → recuperar textos → invocar LLM con contexto. Cada consulta del usuario dispara varias operaciones, lo que puede aumentar ligeramente el tiempo de respuesta. Optimizar para que siga en rangos aceptables (p. ej. usando caché para respuestas frecuentes) será necesario.    
- **Necesidad de Actualizar la Base de Conocimiento:** Aunque no se re-entrene el modelo, sí hay que asegurar que los documentos base se mantienen al día. Si, por ejemplo, una ordenanza es derogada y se aprueba una nueva, se debe cargar el cambio en el repositorio documental. Esto requiere un flujo de mantenimiento (manual o automatizado). Afortunadamente, se pueden establecer procesos automáticos para reindexar documentos modificados periódicamente [aws.amazon.com](https://aws.amazon.com/es/what-is/retrieval-augmented-generation/#:~:text=Actualizar%20datos%20externos).
    

### **Requerimientos Técnicos:**

- **Motor de Búsqueda o Base Vectorial:** Para implementar RAG se podría integrar un servicio como **Elasticsearch** (búsqueda de texto tradicional y/o semántica) o una **base de datos de vectores** como **Qdrant** o **Pinecone** para búsqueda por similitud semántica. Por ejemplo, Qdrant permitiría almacenar embeddings de textos municipales y recuperar rápidamente los más similares a la pregunta del usuario. Los datos a indexar incluirían la FAQ existente, documentos oficiales (PDFs, DOCs convertidos a texto) y posiblemente contenido de la web municipal.
    
- **Generación de Embeddings:** Se necesita un modelo de _embeddings_ para representar preguntas y documentos en vectores numéricos comparables. OpenAI ofrece modelos como `text-embedding-ada-002` o se pueden usar alternativos open-source (Sentence-BERT, etc.). Ya en implementaciones previas se usaba Ada-002 para embeddings de FAQ [GitHub](https://github.com/danielob19/asistente-lic-bustamante/blob/3bc1c79fbb06167c6f858dfbef12263036781657/core/faq_semantica.py#L39-L47). Habría que incorporar un proceso que tome cada documento municipal y calcule sus embeddings, almacenándolos en la base vectorial.
    
- **Pipeline de Preprocesamiento:** Posiblemente un contenedor o script dedicado (por ejemplo, un flujo en `llm_docs-mcp` o un **ETL** externo) que extraiga el texto de fuentes diversas (bases de datos municipales, PDFs escaneados, etc.), limpie el texto y lo ingrese al índice. El archivo `tools/refresh_llm_docs.sh` sugiere la existencia de un mecanismo para refrescar la base de documentación. Esto puede programarse para correr periódicamente o bajo demanda cuando se carguen nuevos documentos (p. ej., vía los endpoints de administración de documentos ya disponibles[GitHub](https://github.com/Publilab/munbot-mcp/blob/dde8c1b06f1e6ec072cd2fb253579f9afd502e6f/mcp-core/orchestrator.py#L855-L864)[GitHub](https://github.com/Publilab/munbot-mcp/blob/dde8c1b06f1e6ec072cd2fb253579f9afd502e6f/mcp-core/orchestrator.py#L923-L931)).
    
- **Cambios en `llm_docs-mcp`:** Este microservicio debería encargarse de las operaciones de búsqueda. Podría exponerse un endpoint (o usar el existente `/tools/call`) para consultas tipo `doc-buscar_fragmento_documento`. Internamente, en vez de una simple búsqueda SQL como ahora, realizaría la búsqueda en la base vectorial o Elastic y devolvería los fragmentos más relevantes. Actualmente, el orquestador ya define la intención `doc-buscar_fragmento_documento` y le asigna el microservicio `llm_docs-mcp` [GitHub](https://github.com/Publilab/munbot-mcp/blob/dde8c1b06f1e6ec072cd2fb253579f9afd502e6f/mcp-core/orchestrator.py#L248-L255) [GitHub](https://github.com/Publilab/munbot-mcp/blob/dde8c1b06f1e6ec072cd2fb253579f9afd502e6f/mcp-core/orchestrator.py#L240-L248), por lo que habría que implementar la lógica correspondiente en `llm_docs-mcp` (posiblemente en su `gateway.py`). El objetivo es que cuando reciba la consulta, extraiga digamos los 3 pasajes más relevantes de la documentación [GitHub](https://github.com/Publilab/munbot-mcp/blob/dde8c1b06f1e6ec072cd2fb253579f9afd502e6f/mcp-core/orchestrator.py#L717-L725).
    
- **Cambios en `mcp-core`:** El orquestador deberá integrar esta funcionalidad de forma transparente. Dado que ya existe una función para obtener “snippets” de contexto tanto de FAQ como de documentos[GitHub](https://github.com/Publilab/munbot-mcp/blob/dde8c1b06f1e6ec072cd2fb253579f9afd502e6f/mcp-core/orchestrator.py#L322-L331) [GitHub](https://github.com/Publilab/munbot-mcp/blob/dde8c1b06f1e6ec072cd2fb253579f9afd502e6f/mcp-core/orchestrator.py#L332-L340), esa podría modificarse para usar el nuevo motor de búsqueda en lugar de (o además de) la consulta SQL sencilla. Alternativamente, el orquestador puede delegar totalmente la búsqueda al microservicio: es decir, cuando la intención sea `doc-buscar_fragmento_documento`, simplemente envía la pregunta a `llm_docs-mcp` y éste retorna los textos encontrados. Luego el orquestador podría decidir si devolver esos fragmentos tal cual o, más potente, invocar al LLM para que genere una respuesta final usando dichos fragmentos (lo que sería la intención `doc-generar_respuesta_llm`). Actualmente, el código ya contempla un flujo donde compone un prompt con los snippets de FAQ/documentos y obtiene una respuesta del LLM [GitHub](https://github.com/Publilab/munbot-mcp/blob/dde8c1b06f1e6ec072cd2fb253579f9afd502e6f/mcp-core/orchestrator.py#L717-L725) [GitHub](https://github.com/Publilab/munbot-mcp/blob/dde8c1b06f1e6ec072cd2fb253579f9afd502e6f/mcp-core/orchestrator.py#L729-L732); esto es básicamente RAG: recuperación de trozos y generación de la respuesta. Con una base documental más robusta, se continuaría ese enfoque, solo que mejorando cómo se obtienen los trozos relevantes.
    
- **Gestión de Contexto e Historial:** Es importante también ajustar el manejo de contexto conversacional. En RAG, a veces la pregunta del usuario puede ser ambigua hasta que se ve en contexto de la conversación. El `ConversationalContextManager` ya almacena el historial reciente en Redis[GitHub](https://github.com/Publilab/munbot-mcp/blob/dde8c1b06f1e6ec072cd2fb253579f9afd502e6f/docs/analisis_estrategia_conversacional_mcp.md#L2-L5), y ese historial se incluye en el prompt[GitHub](https://github.com/Publilab/munbot-mcp/blob/dde8c1b06f1e6ec072cd2fb253579f9afd502e6f/mcp-core/orchestrator.py#L717-L725). Se podría considerar también utilizar un **almacenamiento vectorial para memoria a largo plazo** (mencionado en la arquitectura propuesta[GitHub](https://github.com/Publilab/munbot-mcp/blob/dde8c1b06f1e6ec072cd2fb253579f9afd502e6f/docs/analisis_estrategia_conversacional_mcp.md#L8-L11)), de modo que incluso si el usuario retoma un tema anterior, el sistema pueda recordar qué documentos o info se usó antes. Esto requiere almacenar representaciones de la conversación en la misma base vectorial o en otra, pero es un plus avanzado.
    

**Aplicabilidad en Entorno Municipal:** La generación aumentada con recuperación es **sumamente prometedora** para un chatbot municipal. Las municipalidades manejan gran cantidad de información reglamentaria y de servicios que suele estar en documentos formales o bases de datos. RAG permitiría que el bot brinde respuestas **precisas y fundamentadas**, citando por ejemplo el artículo de una ordenanza o los pasos listados en un trámite oficial. Esto eleva la calidad del servicio, pues el ciudadano recibe información confiable y detallada, en vez de una respuesta general. Además, es escalable: si mañana se agregan nuevos servicios o normativas, se incorporan los documentos y el bot podría atender preguntas sobre ellos sin necesidad de reprogramación. En términos de viabilidad, muchas ciudades ya disponen de sitios web con sus normativas y guías; esas mismas fuentes pueden nutrir el índice de RAG. Como desafío, está la necesidad de mantener actualizado el repositorio (lo cual puede coordinarse con la unidad de comunicaciones o secretaría municipal que emita nuevos contenidos). En resumen, el enfoque RAG equilibra la **naturalidad** de las respuestas generadas con la **exactitud** de las fuentes municipales, lo que es ideal para un asistente virtual público.

_Esquema conceptual de un flujo RAG:_ _El sistema recupera información relevante de la base de conocimientos municipal (documentos, FAQs, datos) y la incorpora al prompt del LLM. El modelo genera una respuesta apoyada en esa información, mejorando precisión y actualidad de la respuesta._


---
---
# Propuesta de Implementación de RAG en MunBot MCP

## 1. Motor de Búsqueda Vectorial Recomendado

Para dotar al sistema de **búsqueda semántica** robusta, se propone incorporar una base de datos vectorial dedicada en lugar del enfoque de texto plano actual. Entre las opciones evaluadas, **Qdrant** destaca como la alternativa más adecuada por varias razones:

- **Facilidad de implementación:** Qdrant es open-source y ofrece un _container_ Docker ligero con API REST y clientes Python listos para usar. Su instalación e integración en Docker Compose sería sencilla (similar a añadir un servicio como Redis o Postgres).
    
- **Rendimiento en cargas bajas/medias:** Al ser un motor especializado en vectores, Qdrant proporciona búsquedas de similitud rápidas con baja latencia incluso en hardware modesto [medium.com](https://medium.com/@mutahar789/optimizing-rag-a-guide-to-choosing-the-right-vector-database-480f71a33139#:~:text=Full,datasets%20employed%20for%20benchmarking%20purposes). Para el volumen actual de documentos (decenas o cientos de páginas de texto) su desempeño será más que suficiente.
    
- **Flexibilidad y soporte activo:** Qdrant permite almacenar **payloads** junto a cada vector (útil para guardar metadatos como ID de documento, título, etc.) y soporta filtrado si se requiere. Cuenta con una comunidad activa y documentación clara. Es open-source y no impone costos.
    
- **Comparativa con alternativas:** Weaviate es otra opción viable (también open-source y con características avanzadas de filtrado híbrido), pero su despliegue añade algo más de complejidad (e.g. requiere módulo de vectorización o uso de GraphQL para consultas). Elasticsearch con búsqueda vectorial, si bien integra la búsqueda semántica en una plataforma conocida, **presenta mayor latencia y menor rendimiento en comparación** con motores vectoriales dedicados [medium.com](https://medium.com/@mutahar789/optimizing-rag-a-guide-to-choosing-the-right-vector-database-480f71a33139#:~:text=Full,datasets%20employed%20for%20benchmarking%20purposes). Dado que actualmente no se usa Elasticsearch en MCP, incorporarlo solo para vectores sería sobre dimensionar la solución. En cambio, **Qdrant ofrece un equilibrio ideal**: ligero, dedicado a vectores y con excelente rendimiento según benchmarks independientes [medium.com](https://medium.com/@mutahar789/optimizing-rag-a-guide-to-choosing-the-right-vector-database-480f71a33139#:~:text=databases%20often%20require%20augmentation%20with,datasets%20employed%20for%20benchmarking%20purposes).
    

En resumen, **se recomienda Qdrant** como base vectorial para implementar RAG. Su despliegue puede ser como un **servicio adicional** en `docker-compose.yml` (por ejemplo, un contenedor `qdrant` escuchando en un puerto interno). La comunicación desde `llm_docs-mcp` se haría vía HTTP (cliente REST) o usando el SDK de Qdrant para Python, según convenga. Esta elección garantiza soporte a futuro (escala horizontal si fuese necesario) y alineamiento con las mejores prácticas RAG actuales [medium.com](https://medium.com/@mutahar789/optimizing-rag-a-guide-to-choosing-the-right-vector-database-480f71a33139#:~:text=Therefore%2C%20for%20retrieval%20augmented%20generation%2C,should%20center%20around%20factors%20such).

## 2. Cambios Propuestos en `llm_docs-mcp` (Indexación y Búsqueda Semántica)

Para transformar `llm_docs-mcp` en un servicio de recuperación semántica, será necesario realizar las siguientes modificaciones:

- **Preprocesamiento e indexación de documentos:** Al inicializar el microservicio, éste debe cargar todos los documentos `.txt` presentes en el contenedor (directorio `documents/`) así como las entradas del FAQ (`faq_respuestas.json`). Cada documento de texto se segmentará en fragmentos manejables (párrafos o bloques de ~100-200 palabras) para aumentar la granularidad de la búsqueda. Igualmente, cada pregunta-respuesta de FAQ se puede tratar como un fragmento independiente. Es recomendable incluir tanto la **pregunta de FAQ como su respuesta** concatenadas en el texto indexado, de modo que la semántica de la pregunta quede capturada (alternativamente, se podría indexar la pregunta por separado y asociarla a su respuesta). Se eliminarán caracteres o espacios innecesarios y se normalizará el texto (por ejemplo, sin saltos de línea excesivos) para optimizar la calidad de los _embeddings_.
    
- **Generación de embeddings:** Se integrará un modelo de _embeddings_ semánticos para convertir cada fragmento de documento/FAQ en un vector numérico. Dado que los textos están en español, se sugiere usar un modelo multilingüe de **Sentence Transformers** (por ejemplo, `paraphrase-multilingual-MiniLM-L12-v2` o similar) que produce vectores de dimensiones ~384-768 con buen rendimiento en español. Este modelo se puede cargar en `llm_docs-mcp` (añadiéndolo a los requisitos e inicializándolo al arranque). Cada fragmento obtendrá su vector y se almacenará en la base vectorial junto con metadatos: identificador de documento, título o categoría (p.ej. indicar si proviene de un documento oficial o de una FAQ). Esta indexación puede realizarse **automáticamente al iniciar** el contenedor (o periódicamente mediante un endpoint administrativo) para que siempre refleje los archivos actuales.
    
- **Almacén vectorial e inserción:** Con Qdrant en funcionamiento, `llm_docs-mcp` creará una colección (p. ej. `munbot_docs`) si no existe, con configuración apropiada (métricas de similitud coseno o dot product). Insertará cada vector con su payload. Por ejemplo, un payload puede tener `{ "doc_id": "...", "titulo": "...", "texto": "..." }` o similar; aunque no es necesario almacenar el texto completo si lo mantenemos también en memoria. Alternativamente, se puede guardar solo el vector y un ID, y mantener un diccionario en memoria de ID->texto fragmento para reconstruir la respuesta rápidamente.
    
- **Implementación de la búsqueda semántica:** Se añadirá lógica para, dada una consulta del usuario, **generar el embedding de la pregunta** con el mismo modelo y buscar en la base vectorial los _k_ vectores más similares. Qdrant permite realizar esta búsqueda con una simple llamada (por distancia coseno si los embeddings están normalizados). Se aplicará un umbral de similitud para evitar retornos irrelevantes: por ejemplo, si la puntuación de similitud más alta es muy baja, el sistema podría devolver ningún fragmento (lo que indicaría que la pregunta es fuera de dominio). En pruebas iniciales con datos reales se ajustará este umbral; típicamente un coseno < 0.3-0.4 podría considerarse no informativo, mientras que valores cercanos a 1.0 indican alta relevancia.
    
- **Nuevo endpoint para recuperar fragmentos:** Actualmente, `llm_docs-mcp` expone un endpoint genérico `/tools/call` para recibir peticiones de herramientas[GitHub](https://github.com/Publilab/munbot-mcp/blob/dde8c1b06f1e6ec072cd2fb253579f9afd502e6f/services/llm_docs-mcp/gateway.py#L134-L141). Se debe implementar dentro de este endpoint la rama para la herramienta `doc-buscar_fragmento_documento`. Según el esquema JSON definido[GitHub](https://github.com/Publilab/munbot-mcp/blob/dde8c1b06f1e6ec072cd2fb253579f9afd502e6f/mcp-core/tool_schemas/doc-buscar_fragmento_documento.json#L26-L34), al recibir `{"tool": "doc-buscar_fragmento_documento", "params": {"consulta": "...", "k": N}}`, el servicio realizará la búsqueda semántica descrita: obtener _N_ fragmentos relevantes a la _consulta_. Por cada fragmento encontrado, se construirá un objeto con:
    
    - `doc_id`: un identificador del documento fuente (p.ej., podría ser el nombre de archivo `.txt` o un ID interno).
        
    - `titulo`: título o nombre del documento (p.ej., si el `.txt` tiene un título conocido o se puede derivar del nombre de archivo; en el caso de FAQs, podría ser una etiqueta como `"FAQ"` o el texto de la pregunta).
        
    - `parrafo`: el contenido del fragmento de texto recuperado.
        
    - `puntaje`: la puntuación de similitud o relevancia del fragmento (opcionalmente normalizada entre 0 y 1, o se puede devolver la distancia inversa). Este campo es útil para depuración o para que el orquestador decida umbrales, aunque no necesariamente se mostrará al usuario final.
        
    
    El endpoint entonces devolverá un JSON con la lista de `fragmentos` encontrados. Por ejemplo: `{"fragmentos": [ { "doc_id": "licencia_conducir.txt", "titulo": "Licencia de Conducir", "parrafo": "Para obtener la licencia de conducir debe ...", "puntaje": 0.87}, {...}, ... ] }`.
    
    Esto alinea la implementación con el contrato definido en `doc-buscar_fragmento_documento.json`[GitHub](https://github.com/Publilab/munbot-mcp/blob/dde8c1b06f1e6ec072cd2fb253579f9afd502e6f/mcp-core/tool_schemas/doc-buscar_fragmento_documento.json#L26-L34). Cabe destacar que actualmente `llm_docs-mcp` ya maneja herramientas similares (`buscar_documento_por_tag`, etc.), por lo que añadiremos una condición más en su lógica de routing. Por ejemplo:
    
```python
if tool == "doc-buscar_fragmento_documento":
    consulta = params["consulta"]
    k = params.get("k", 3)
    # [Realizar búsqueda vectorial de consulta en la colección]
    # Construir lista de fragmentos como se describió...
    return {"fragmentos": lista_fragmentos}
```
    
En caso de que la consulta no encuentre ningún fragmento relevante (p.ej. similitud por debajo del umbral), se puede retornar `{"fragmentos": []}` o incluso un código de error/mensaje indicando que no hay información, aunque probablemente la mejor decisión es retornar lista vacía y dejar que el orquestador maneje esa situación.
    
- **Indexación automática y actualización:** Para mantener la **sostenibilidad futura**, `llm_docs-mcp` debería contar con un mecanismo de reindexar cuando los datos cambien. Dado que los documentos y FAQs están en volumen (montados en el contenedor), se podría ejecutar el proceso de indexación cada vez que se inicia el servicio (lo cual garantiza frescura si el contenedor se reconstruye con nuevos archivos). Si se espera actualizaciones frecuentes sin reinicios, se puede considerar implementar un endpoint administrativo (no expuesto públicamente) para, por ejemplo, volver a cargar archivos e reindexar en caliente. No obstante, inicialmente la reindexación en arranque más la posibilidad de reconstruir la imagen cuando haya cambios puede ser suficiente, dado que el volumen de documentos municipales no cambia a diario.
    

En resumen, estos cambios convertirán `llm_docs-mcp` en un _vector store service_ capaz de mapear consultas a fragmentos relevantes mediante similitud semántica, en lugar de por coincidencia literal de palabras. Esto soluciona las limitaciones actuales de la búsqueda por tokens (que podría fallar ante preguntas parafraseadas) y sienta las bases para respuestas fundamentadas en el conocimiento interno.

## 3. Cambios en `mcp-core` (Orquestación con Recuperación Semántica)

En el núcleo orquestador (`mcp-core/orchestrator.py`), los cambios estarán dirigidos a **delegar la fase de recuperación de información** al microservicio `llm_docs-mcp` y a integrar las respuestas del modelo de lenguaje con esa información. Actualmente, el flujo para consultas generales (no estructuradas) es como sigue:

- Si la entrada del usuario coincide con una FAQ (exacta o parcialmente), se devuelve directamente la respuesta predefinida[GitHub](https://github.com/Publilab/munbot-mcp/blob/dde8c1b06f1e6ec072cd2fb253579f9afd502e6f/mcp-core/orchestrator.py#L283-L291)[GitHub](https://github.com/Publilab/munbot-mcp/blob/dde8c1b06f1e6ec072cd2fb253579f9afd502e6f/mcp-core/orchestrator.py#L566-L574).
    
- Si no hay coincidencia FAQ ni intención de herramienta conocida, el orquestador obtiene fragmentos de contexto con `retrieve_context_snippets` –que usa coincidencia de palabras clave en FAQs y búsqueda SQL LIKE en los documentos oficiales[GitHub](https://github.com/Publilab/munbot-mcp/blob/dde8c1b06f1e6ec072cd2fb253579f9afd502e6f/mcp-core/orchestrator.py#L304-L313)[GitHub](https://github.com/Publilab/munbot-mcp/blob/dde8c1b06f1e6ec072cd2fb253579f9afd502e6f/mcp-core/orchestrator.py#L322-L331)– y luego forma un prompt para el LLM con esos fragmentos[GitHub](https://github.com/Publilab/munbot-mcp/blob/dde8c1b06f1e6ec072cd2fb253579f9afd502e6f/mcp-core/orchestrator.py#L717-L725).
    
- El modelo Mistral genera entonces una respuesta abierta usando ese contexto.
    

Con la introducción del sistema RAG, ajustaremos este flujo de la siguiente manera:

- **Delegación de búsqueda al microservicio:** En lugar de usar `retrieve_context_snippets` (que se volverá obsoleto), el orquestador hará una **llamada HTTP** al endpoint de `llm_docs-mcp` para obtener los fragmentos relevantes. Concretamente, cuando se determine que la consulta es una pregunta abierta/informativa (intención general), se invocará `call_tool_microservice("doc-buscar_fragmento_documento", {"consulta": pregunta, "k": 3})`[GitHub](https://github.com/Publilab/munbot-mcp/blob/dde8c1b06f1e6ec072cd2fb253579f9afd502e6f/mcp-core/orchestrator.py#L146-L155). La URL de este servicio ya está configurada en la variable de entorno `LLM_DOCS_MCP_URL` y en el diccionario `MICROSERVICES` del orquestador[GitHub](https://github.com/Publilab/munbot-mcp/blob/dde8c1b06f1e6ec072cd2fb253579f9afd502e6f/mcp-core/orchestrator.py#L20-L28)[GitHub](https://github.com/Publilab/munbot-mcp/blob/dde8c1b06f1e6ec072cd2fb253579f9afd502e6f/docs/analisis_estrategia_conversacional_mcp.md#L110-L118), apuntando a la ruta `/tools/call` del microservicio. Esto asegurará que usamos la lógica semántica recién implementada.
    
- **Integración con el flujo de intención:** Cabe mencionar que el sistema de detección de intención (potenciado por el modelo Mistral de clasificación) ya contempla acciones como `doc-buscar_fragmento_documento` y `doc-generar_respuesta_llm`[GitHub](https://github.com/Publilab/munbot-mcp/blob/dde8c1b06f1e6ec072cd2fb253579f9afd502e6f/docs/analisis_estrategia_conversacional_mcp.md#L138-L145). Es decir, el LLM puede inferir que ante cierta pregunta del usuario, debe activarse la búsqueda documental. Aprovechando esto:
    
    - Si la **detección de intención** retorna `tool = "doc-buscar_fragmento_documento"` para la consulta, el orquestador procederá directamente a llamar a `llm_docs-mcp` como se describió. Alternativamente, dado que en la implementación actual cualquier consulta desconocida cae en la rama `"unknown" o "doc-generar_respuesta_llm"`[GitHub](https://github.com/Publilab/munbot-mcp/blob/dde8c1b06f1e6ec072cd2fb253579f9afd502e6f/mcp-core/orchestrator.py#L711-L719), podemos simplificar y dirigir ambos casos a la lógica RAG (efectivamente, tratar `unknown` como necesidad de RAG).
        
    - Se puede retirar o modificar la lógica de `lookup_faq_respuesta` inicial[GitHub](https://github.com/Publilab/munbot-mcp/blob/dde8c1b06f1e6ec072cd2fb253579f9afd502e6f/mcp-core/orchestrator.py#L566-L574). Dado que queremos que **todas las respuestas generales las genere el modelo** Mistral apoyándose en documentos/FAQ, podría dejarse de responder con texto fijo de FAQ. En su lugar, incluso las preguntas frecuentes pasarían por el flujo de RAG (lo que permite que el modelo formule la respuesta manteniendo el tono consistente). Sin embargo, como medida de respaldo, podríamos conservar una verificación de coincidencia exacta de FAQ para casos muy obvios – por ejemplo, si la pregunta del usuario coincide literalmente con una FAQ, se puede retornar directamente la respuesta pre-escrita para garantizar precisión absoluta. Esto sería un atajo opcional; en la propuesta asumiremos el camino principal de usar siempre el modelo con contexto, salvo que se decida lo contrario.
        
    - En cualquier caso, se mantendrá la lógica de **slot-filling** para reclamos y agendamientos tal cual está, ya que esas son interacciones estructuradas aparte del RAG. Es decir, la introducción de RAG afecta principalmente el manejo de intenciones "doc-" (documentación) y las previamente no reconocidas.
        
- **Composición del _prompt_ con los fragmentos:** Una vez obtenidos los fragmentos desde `llm_docs-mcp`, el orquestador debe construir el mensaje de entrada para el modelo Mistral. Esto seguirá el patrón ya existente con el prompt `doc-generar_respuesta_llm.txt`[GitHub](https://github.com/Publilab/munbot-mcp/blob/dde8c1b06f1e6ec072cd2fb253579f9afd502e6f/mcp-core/orchestrator.py#L719-L728), pero asegurándonos de incorporar los nuevos fragmentos. En concreto:
    
    - Tomaremos la lista de fragmentos devueltos (campo `"fragmentos"` del JSON). Extraeremos de cada objeto el texto del `parrafo` (y potencialmente el `titulo` para darle contexto adicional).
        
    - Prepararemos el contexto como un bloque de texto unificado. Una opción clara es enumerar o citar los fragmentos, por ejemplo:
        
```yaml
Información relevante encontrada:
[Documento: {titulo1}] {parrafo1}
[Documento: {titulo2}] {parrafo2}
...
```
        
Si los títulos de documentos son significativos, incluyen el tema (p.ej. "Permiso de Circulación"), ayudarán al modelo a entender la fuente de cada fragmento. En caso de fragmentos provenientes de FAQ, el "título" podría ser la propia pregunta original de la FAQ para mayor claridad.
        
- Este bloque de contexto se insertará en el prompt template en el lugar correspondiente (en la plantilla actual se usa la variable `{faq_context}` para los fragmentos[GitHub](https://github.com/Publilab/munbot-mcp/blob/dde8c1b06f1e6ec072cd2fb253579f9afd502e6f/mcp-core/orchestrator.py#L722-L728), que podemos reutilizar para cualquier contexto documental). También se insertará la pregunta del usuario en el placeholder `{pregunta}`.
        
 - Adicionalmente, ajustaremos las **instrucciones del prompt** para enfatizar que el modelo **debe usar únicamente** la información proporcionada en esos fragmentos al generar la respuesta. Por ejemplo, la plantilla podría decir al inicio: _"Eres un asistente de ayuda municipal. Responde a la pregunta del usuario usando **solo** la información siguiente. Si la información no es suficiente o no está relacionada, indica que no dispones de datos. Mantén un tono amable y formal."_ Esto recordará al modelo que no "alucine" contenido externo.
        
- En cuanto al lenguaje, se puede continuar especificando `{language}` = "es" en el prompt como se hace ahora, para reforzar que la respuesta debe ser en español (aunque el modelo por defecto responderá en el idioma de la pregunta, no está de más).
        
- **Llamada al modelo Mistral:** Después de formar el prompt enriquecido con contexto, el orquestador usará el cliente de Mistral (`MistralClient.generate` o similar) para obtener la respuesta final[GitHub](https://github.com/Publilab/munbot-mcp/blob/dde8c1b06f1e6ec072cd2fb253579f9afd502e6f/services/llm_docs-mcp/gateway.py#L124-L132). Este es el **motor generativo** que transformará el contexto en la respuesta en lenguaje natural. Dado que ahora todas las respuestas generales pasan por aquí, se debe asegurar que los parámetros de generación sean seguros: por ejemplo, usar una **temperatura baja** (ej. 0.2-0.7) y quizás top_p/top_k moderados para privilegiar respuestas precisas sobre creativas. Así nos cercioramos de que el modelo no invente datos fuera del contexto.
    
- **Devolución de la respuesta al usuario:** El orquestador enviará la respuesta generada dentro del JSON de salida, tal como hace actualmente (clave `"respuesta"`). Adicionalmente, seguirá manteniendo el `session_id` y actualizando el contexto conversacional en Redis para memoria a corto plazo. Notar que en la implementación actual, después de generar la respuesta con el LLM, el orquestador la guarda en el historial de contexto[GitHub](https://github.com/Publilab/munbot-mcp/blob/dde8c1b06f1e6ec072cd2fb253579f9afd502e6f/mcp-core/orchestrator.py#L730-L738), lo cual está bien para continuidad. Esto no cambia con RAG, salvo que ahora esas respuestas estarán fundamentadas.
    

Con estos ajustes, `mcp-core` básicamente **actuará como coordinador RAG**: detecta la necesidad de búsqueda, obtiene conocimiento pertinente vía `llm_docs-mcp` y construye la pregunta final para el LLM. Esto mantiene la arquitectura modular: la lógica de búsqueda permanece aislada en el microservicio (fácil de modificar o escalar si fuese necesario), mientras el orquestador sigue manejando la conversación y otras rutas de intención (reclamos, citas, etc.) sin mezclarlas con la capa de documentación.

## 4. Detalles Técnicos: Prompts, Umbrales y Flujo de Procesamiento

A continuación, se profundiza en algunos detalles de implementación para garantizar que el sistema RAG funcione óptimamente:

- **Formato de los prompts con contexto:** Es vital estructurar el prompt de entrada al LLM de manera clara. Se recomienda un formato delimitado, por ejemplo usando secciones identificadas:
    
    - **Contexto:** incluir aquí los fragmentos recuperados. Cada fragmento podría aparecer como viñeta o párrafo citado. Usar algún indicador de origen (título o ID del documento) ayuda a dar credibilidad y a que el modelo no mezcle datos de distintas fuentes.
        
    - **Pregunta del usuario:** después del contexto, se escribe literalmente la pregunta original del usuario.
        
    - **Instrucción final al modelo:** pedir explícitamente que responda usando solo el contexto proporcionado. Por ejemplo: _"Responde a la pregunta utilizando únicamente la información anterior. Si la pregunta no puede ser respondida con ese contexto, indica al usuario que no dispones de esa información."_ De esta forma, el modelo entenderá los límites de conocimiento para esta respuesta.
        
    
    Un ejemplo sintetizado podría ser:
    
```plaintext
[CONTEXT]
Documento "Licencia de Conducir": Para obtener la licencia de conducir, debe presentar su cédula...
FAQ "¿Dónde pago mi permiso?": Puede pagar su permiso de circulación en la tesorería municipal...

[USER QUESTION]
¿Qué requisitos necesito para la licencia de conducir y dónde la puedo obtener?

[INSTRUCTION]
Responde en español claro y conciso, basado en el contexto proporcionado arriba. No agregues información que no esté en el contexto.    
 ```
        
Este prompt le da al modelo toda la evidencia necesaria y le ordena ceñirse a ella.
    
- **Umbrales de recuperación y cantidad de fragmentos (k):** En el esquema se planteó _k_ por defecto = 3[GitHub](https://github.com/Publilab/munbot-mcp/blob/dde8c1b06f1e6ec072cd2fb253579f9afd502e6f/mcp-core/tool_schemas/doc-buscar_fragmento_documento.json#L14-L22), lo cual suele ser un buen punto de partida (3 fragmentos). Se podría permitir ajustar _k_ en la petición, pero en la práctica el orquestador probablemente siempre pida un número fijo (3 o 5). Es importante no inundar al modelo con demasiado texto irrelevante: incluir más fragmentos que los necesarios puede aumentar el ruido. Por ello:
    
    - Se debe experimentar con el **umbral de similitud** para filtrar fragmentos. Por ejemplo, si la similitud coseno máxima para la consulta es < 0.3, quizás ningún fragmento sea realmente útil y podríamos considerar que no hay respuesta en la base de conocimiento. En tal caso, el sistema podría desencadenar un mensaje de “no se encontró información” en lugar de arriesgar una alucinación.
        
    - Si el primer fragmento tiene alta similitud y los siguientes tienen puntajes muy inferiores, podría optarse por devolver solo el primero. Sin embargo, a veces una pregunta cubre dos aspectos distintos y los siguientes fragmentos pueden complementar la respuesta. Un criterio balanceado es: _incluir todos los fragmentos por encima de cierto umbral relativo_. Por ejemplo, tomar todos aquellos con puntuación dentro del ~80% del máximo, hasta un máximo de k.
        
    - En la primera iteración, una **implementación simple** es siempre devolver los top _k_ fragmentos ordenados por relevancia (aunque alguno sea marginalmente útil). Luego, mediante pruebas, ajustar: si el modelo tiende a confundirse con información irrelevante, se sube el umbral o se reduce k.
        
- **Limpieza y normalización de documentos:** Durante la indexación, conviene realizar algunos pasos de limpieza:
    
    - Remover contenido redundante (por ejemplo, encabezados o pie de página repetitivos en documentos oficiales que no aportan significado).
        
    - Convertir todo a minúsculas para la fase de embedding, **excepto** en casos donde la mayúscula distinga nombres propios que pudieran ser relevantes (aunque los modelos de embedding generalmente no son sensibles a caso). También quitar acentos podría ser opcional; los modelos multilingües suelen manejar acentos correctamente, así que esto no es estrictamente necesario.
        
    - Si los documentos contienen listas muy largas o tablas, podría considerarse segmentarlos diferente. Por ejemplo, cada ítem de lista como fragmento separado, si son autónomos. La idea es maximizar la probabilidad de que un fragmento contenga la respuesta exacta a algo.
        
    - **Metadatos:** A cada fragmento, asignar como metadato el título o categoría. Por ejemplo, si un `.txt` corresponde a un trámite específico, ese nombre puede almacenarse y eventualmente incluirse en la respuesta (puede darle más peso y claridad a la respuesta del bot, e.j. "Para el trámite _Licencia de Conducir_, necesita..."). Incluso podríamos incluir el nombre del documento en el prompt entre corchetes para que el modelo pueda citarlo en la respuesta si cuadra, aunque habría que vigilar que no “hable” como robot citando IDs. Posiblemente, el modelo usará esa info tácitamente para contextualizar.
        
- **Estructura de los embeddings e índice:** Se recomienda indexar **a nivel de fragmento** y no del documento completo. Así, cada entrada del vector store corresponde a un párrafo o sección pequeña. Esto mejora la resolución de la búsqueda. Cada vector lleva un ID único; podemos usar una convención como `{doc_id}#{sec_num}` o similar para codificar a qué documento y sección pertenece. Así, por ejemplo, `"licencia.txt#3"` podría indicar el tercer fragmento del archivo licencia.txt. Este ID puede devolverse como `doc_id` en los resultados[GitHub](https://github.com/Publilab/munbot-mcp/blob/dde8c1b06f1e6ec072cd2fb253579f9afd502e6f/mcp-core/tool_schemas/doc-buscar_fragmento_documento.json#L32-L39), y en el campo `titulo` simplemente el nombre del documento ("Licencia de Conducir") para mostrar.
    
    - Si en el futuro se requiere escalar a muchos documentos, Qdrant permite particionar por colecciones o usar filtrado por metadato (por ejemplo, podríamos marcar con metadato `{"tipo": "faq"}` o `{"tipo": "doc_oficial"}` y filtrar si quisiéramos buscar solo en uno u otro). Inicialmente, no parece necesario separar, ya que el volumen total es manejable y buscar en todo el conjunto unificado facilita encontrar tanto respuestas de documentación oficial como de FAQ con una sola consulta.
        
    - Asegurarse de elegir la **métrica de similitud** apropiada al crear la colección en Qdrant. Si usamos embeddings normalizados (Sentence Transformers suele producir vectores normalizados en muchos casos), la _distancia coseno_ es conveniente. Alternativamente, puede usarse _dot product_ pero entonces hay que garantizar la normalización manual. La métrica coseno permite interpretar el `score` de Qdrant directamente como similitud 0-1 aproximadamente.
        
- **Flujo completo de procesamiento (resumen):** Para clarificar cómo se conectan las piezas tras estos cambios, describimos paso a paso:
    
    1. **Entrada del usuario:** El usuario hace una pregunta general, por ejemplo: "¿Cuáles son los requisitos para obtener una licencia de conducir?".
        
    2. **Orquestador (mcp-core):** Recibe la pregunta en `/orchestrate`. El sistema de intención (Mistral interno de clasificación) la analiza. Dada la naturaleza de la pregunta, probablemente asigna la intención `doc-buscar_fragmento_documento` (o simplemente no reconoce ninguna intención de las reglas, categorizándola como desconocida/conversacional).
        
    3. **Llamada a llm_docs-mcp:** El orquestador, al ver que no es un flujo estructurado (reclamo/cita) y requiere información, invoca `llm_docs-mcp` vía `tools/call` con la consulta. Por ejemplo, hace `POST http://llm_docs-mcp:8000/tools/call` con cuerpo `{"tool": "doc-buscar_fragmento_documento", "params": {"consulta": "¿Cuáles son los requisitos para obtener una licencia de conducir?", "k": 3}}`.
        
    4. **Búsqueda vectorial:** El microservicio recibe esta solicitud. Internamente, genera el embedding de la pregunta y busca en la colección vectorial los 3 fragmentos más similares. Supongamos que encuentra fragmentos en el documento "Licencia de Conducir.txt" que enumeran los requisitos, y quizá otro fragmento en un FAQ sobre licencias que menciona dónde se realiza el trámite.
        
    5. **Respuesta de llm_docs-mcp:** Devuelve al orquestador un JSON con los fragmentos, por ejemplo:
        
```json
{
  "fragmentos": [
    {"doc_id": "licencia_conducir.txt#5", "titulo": "Licencia de Conducir", "parrafo": "Los requisitos para obtener la licencia de conducir son: 1) Cédula de identidad vigente, 2) Certificado de antecedentes...", "puntaje": 0.95},
    {"doc_id": "licencia_conducir.txt#7", "titulo": "Licencia de Conducir", "parrafo": "El trámite se realiza en la Dirección de Tránsito ubicada en...", "puntaje": 0.90},
    {"doc_id": "faq_licencia_renovar#1", "titulo": "FAQ Licencia", "parrafo": "Pregunta: ¿Dónde puedo obtener/renovar mi licencia de conducir? Respuesta: En la Dirección de Tránsito de la municipalidad...", "puntaje": 0.85}
  ]
}
```
        
   (La estructura exacta podría variar, pero se ilustra la idea).
        
6. **Construcción del prompt en orquestador:** El orquestador toma estos fragmentos y forma el prompt para Mistral. Une los párrafos en una sección de contexto. Podría quedar algo así:
        
> **Contexto:** [Licencia de Conducir] Los requisitos para obtener la licencia de conducir son: 1) Cédula de identidad vigente, 2) Certificado de antecedentes...  
> [Licencia de Conducir] El trámite se realiza en la Dirección de Tránsito ubicada en...  
> [FAQ Licencia] En la Dirección de Tránsito de la municipalidad se puede obtener/renovar la licencia...  
> **Pregunta:** ¿Cuáles son los requisitos para obtener una licencia de conducir?  
> **Instrucciones:** Responde al usuario utilizando **solo** la información del contexto. Si falta información para responder completamente, indica que no la tienes. Mantén un tono formal pero cercano, como funcionario municipal.
        
Nótese que hemos incluido los fragmentos relevantes, y repetido la pregunta para que el modelo enfoque su respuesta.
        
7. **Generación con Mistral:** Se envía este prompt al modelo Mistral (vía `mistral_client.generate`). El modelo procesará el contexto y la pregunta, y generará una respuesta. Idealmente dirá algo como: _"Para obtener una licencia de conducir necesitas presentar tu cédula de identidad vigente, un certificado de antecedentes y otros documentos. Este trámite se realiza en la Dirección de Tránsito de la municipalidad."_ – que efectivamente está **basada en los fragmentos proporcionados**.
        
8. **Respuesta al usuario:** El orquestador recibe la respuesta del modelo, la registra en el historial (Redis/Postgres) y la envía al frontend (`web-interface` u otra integración) en formato JSON: `{"respuesta": "<texto generado>", "session_id": "..."}.` El usuario ve la respuesta en el chat del bot.
        
    Si en algún paso la base de conocimiento no tuviera nada relevante (por ejemplo, pregunta totalmente fuera de dominio), entonces tras el paso 5 la lista de fragmentos vendría vacía o con puntajes muy bajos. El orquestador podría detectarlo y en vez de llamar al LLM, devolver un mensaje de disculpa o **escalar a fallback humano** (como ya contemplaba el diseño original con la bandera `escalado`: _"Derivaré tu consulta a un agente humano"_[GitHub](https://github.com/Publilab/munbot-mcp/blob/dde8c1b06f1e6ec072cd2fb253579f9afd502e6f/mcp-core/orchestrator.py#L701-L709)). Esta decisión de fallback se puede basar en `if not fragmentos: ...` o en un puntaje debajo de umbral. Es importante definir esto para que el bot no devuelva respuestas vacías o confusas.
    
- **Consideraciones de rendimiento:** Dado que cada pregunta desencadenará una búsqueda vectorial + generación de texto, es importante que ambas operaciones sean eficientes. La búsqueda en Qdrant con pocos miles de vectores es prácticamente instantánea (ms), y la generación con Mistral (7B) vía API de HuggingFace puede tardar del orden de 1-3 segundos por respuesta típica de unas pocas frases (dependiendo de parámetros). En total, esperamos respuestas en pocos segundos, lo cual es aceptable. Para mantener esto:
    
    - Se puede limitar el tamaño de contexto a, digamos, máximo ~1000 tokens sumando todos los fragmentos, para no sobrepasar la ventana de contexto del modelo (Mistral 7B normalmente maneja hasta 2048 tokens). Con 3 fragmentos de ~100 palabras cada uno + la pregunta + instrucciones, estaremos bien dentro del límite.
        
    - Opcionalmente, si el desempeño es crítico, podríamos paralelizar la generación de embeddings de todos los documentos al inicio usando múltiples hilos (el env var N_THREADS ya está considerado[GitHub](https://github.com/Publilab/munbot-mcp/blob/dde8c1b06f1e6ec072cd2fb253579f9afd502e6f/services/llm_docs-mcp/gateway.py#L20-L28)). También podríamos cachear embeddings de preguntas frecuentes comunes si notamos repetición, aunque eso es optimización fina.
        
    - El acceso a `llm_docs-mcp` agrega un pequeño overhead de red (llamada HTTP interna), pero en una red local Docker es muy bajo. A cambio, ganamos separación de responsabilidades.
        

## 5. Buenas Prácticas para Evitar Alucinaciones y Mantener el Tono

Finalmente, es crucial establecer **buenas prácticas** en el diseño de prompt y en la configuración del modelo para asegurar que el chatbot responda con información veraz (no inventada) y en un tono apropiado para un contexto municipal. Recomendaciones concretas:

- **Instrucciones anti-alucinación:** Como se mencionó, incluir en el prompt una instrucción explícita de "no aportar datos fuera del contexto" es fundamental. El modelo debe actuar casi como un sistema de _open-book QA_, donde cualquier contenido de su respuesta proviene de las fuentes proporcionadas. Si la pregunta del usuario contiene detalles no cubiertos en los fragmentos, el bot debería responder con una frase de disculpa o derivación. Por ejemplo: _"Lo siento, no dispongo de esa información en este momento."_ Es mejor esto a que el modelo intente adivinar. En tests internos, se puede comprobar que efectivamente el modelo no usa conocimientos fuera de los fragmentos – dado que Mistral 7B tiene conocimiento limitado (y con cutoff antiguo), apoyarse en los documentos actuales evita respuestas desactualizadas o incorrectas.
    
- **Veracidad y chequeo de fuentes:** Una práctica adicional podría ser que el modelo indique de dónde sale la información (por ejemplo, mencionando "según la Dirección de Tránsito..." si ese dato estaba en un fragmento). Sin embargo, no queremos que el chatbot se vuelva excesivamente formal o cite documentos literalmente, ya que debe sonar natural. Posiblemente, mantener el título del documento en el contexto influirá implícitamente. No recomendamos que el bot liste los IDs de documento al usuario ni nada por el estilo, solo que formule la respuesta con seguridad. Internamente, para verificar, el equipo de desarrollo puede habilitar logs que muestren qué fragmentos se usaron para cada respuesta, facilitando auditoría.
    
- **Ajustar el tono conversacional:** En cuanto al **tono**, las respuestas deben ser informativas pero cercanas y corteses, alineadas con el trato ciudadano. Ya en los flujos estructurados se usa un tono de voz amigable, tuteando al usuario ("Genial, ¿puedes darme tu RUT?"). Conviene que las respuestas generadas por Mistral sigan ese mismo estilo. Para lograrlo, podemos incluir en el prompt alguna indicación de estilo, por ejemplo: _"Responde de manera cordial y clara, dirigiéndote al usuario de 'tú'."_ Mistral 7B-Instruct generalmente produce respuestas educadas por defecto, pero es útil especificar nivel de formalidad. También se puede mostrar en el contexto algún ejemplo de respuesta previa si se tuviera (aunque probablemente no sea necesario entrenarlo de esta forma, con la instrucción textual basta).
    
    Otra buena práctica es **limitar la longitud de la respuesta** para que sea concisa. El parámetro `max_new_tokens` ya se establece (256 en HF API según la documentación[GitHub](https://github.com/Publilab/munbot-mcp/blob/dde8c1b06f1e6ec072cd2fb253579f9afd502e6f/docs/analisis_estrategia_conversacional_mcp.md#L129-L137)), lo cual impide que se extienda demasiado. Aún así, en la instrucción podemos pedir "responde de forma concisa". Si una respuesta requiere enumerar pasos o documentos, el bot debería hacerlo en formato de lista si es apropiado. El LLM, al ver enumeraciones en los fragmentos, probablemente replicará el formato (ej. "1)... 2)..."), lo cual está bien siempre que esté basado en la fuente.
    
- **Validación y pruebas:** Antes de desplegar en producción, se deben **probar múltiples preguntas** (incluyendo variaciones de redacción, errores ortográficos menores, etc.) para verificar que:
    
    - El sistema recupera fragmentos relevantes (inspeccionar lo que `llm_docs-mcp` devuelve).
        
    - El modelo responde correctamente fundamentado en esos fragmentos.
        
    - No agregue datos que suenen plausibles pero no estén respaldados.
        
    - El tono sea el esperado (ni demasiado rígido ni demasiado coloquial).
        
    - Preguntas fuera del dominio (ej. "¿Quién es el presidente de Chile?") provoquen una respuesta de disculpa o derivación, en lugar de cualquier invención.
        
    
    Estas pruebas permitirán afinar umbrales, ajustar el prompt o incluso añadir filtros. Por ejemplo, podríamos detectar si el modelo empieza una respuesta con algo no deseado y ajustar la instrucción. También es recomendable involucrar a algún funcionario municipal en la validación para asegurarse de que las respuestas cumplen con el estándar de calidad y cortesía institucional.
    
- **Monitorización en producción:** Una vez implementado, se puede loguear cada pregunta, los fragmentos recuperados y la respuesta dada (no de cara al usuario, pero sí en logs internos anonimizados). Esto ayudará a identificar casos donde el modelo pudo fallar o donde la base de conocimiento quizá necesite ampliarse. Por ejemplo, si muchas consultas no encuentran fragmentos y activan el fallback de humano, tal vez haya que agregar esas preguntas al FAQ o al documento base en futuras actualizaciones.
    
- **Actualización de documentos:** Mantener el **índice vectorial actualizado** es también una buena práctica para evitar desinformación. Si se publica un nuevo documento municipal (por ejemplo, cambia una normativa), al agregar el nuevo `.txt` al contenedor y re-desplegar, el sistema automáticamente lo indexará. Se podría incluso automatizar una tarea (por ejemplo, un script CI/CD o cron job) para regenerar la imagen/servicio `llm_docs-mcp` semanal o mensualmente con los últimos documentos oficiales, de modo que el chatbot siempre esté al día. Dado que los documentos municipales (ordenanzas, requisitos) pueden cambiar, esto es importante para la **sostenibilidad** del conocimiento.
    

En conclusión, con estos cambios el chatbot MunBot MCP pasará de buscar respuestas por coincidencia literal a utilizar un enfoque **RAG (Retrieval-Augmented Generation)** robusto. Esto le permitirá responder de forma más inteligente y fundamentada, usando la documentación y FAQs existentes como base de conocimiento. La arquitectura propuesta mantiene la coherencia con el diseño modular de MCP: `llm_docs-mcp` actúa como proveedor de información semántica y `mcp-core` sigue orquestando las interacciones. Además, sienta las bases para escalar en el futuro (más documentos, más tipos de consultas) simplemente extendiendo el índice vectorial, sin cambios drásticos en la lógica conversacional. Siguiendo las buenas prácticas descritas, esperamos minimizar alucinaciones del LLM y brindar a los usuarios respuestas confiables, actualizadas y con el tono adecuado de un asistente municipal. Esto representará una mejora significativa sobre el sistema actual de coincidencia de texto plano, llevando a MunBot MCP al siguiente nivel en capacidad conversacional y utilidad para la comunidad. [medium.com](https://medium.com/@mutahar789/optimizing-rag-a-guide-to-choosing-the-right-vector-database-480f71a33139#:~:text=Full,datasets%20employed%20for%20benchmarking%20purposes)[medium.com](https://medium.com/@mutahar789/optimizing-rag-a-guide-to-choosing-the-right-vector-database-480f71a33139#:~:text=Therefore%2C%20for%20retrieval%20augmented%20generation%2C,should%20center%20around%20factors%20such)